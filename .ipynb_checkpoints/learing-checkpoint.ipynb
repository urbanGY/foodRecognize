{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sfsfk\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4476538698732603985\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1422723891\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17569066025973941024\n",
      "physical_device_desc: \"device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "tensorflow version :  2.0.0\n",
      "is gpu available??? :  True\n"
     ]
    }
   ],
   "source": [
    "from imgFetch import get_iterator_next_element\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from layers.conv_layer import conv_layer\n",
    "from layers.max_pool import max_pool\n",
    "from layers.avg_pool import avg_pool\n",
    "from layers.fc_layer import fc_layer\n",
    "\n",
    "from layers.inception import inception_A, inception_B, inception_C\n",
    "from layers.reduction import reduction_A, reduction_B\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print('tensorflow version : ', tf.__version__)\n",
    "print('is gpu available??? : ',tf.test.is_gpu_available())\n",
    "\n",
    "# reuse를 사용하면 재사용이 된다. 그런데 여기에 그게 들어가는게 맞는지 모르겠음;\n",
    "# graph로 variable보는법 알아서 나중에 어떻게 돌아가나 판단\n",
    "\n",
    "# 여긴 나중에 다시 체크하는게 좋을 듯\n",
    "def dcnn(x, keep_prob, img_result):\n",
    "    # stem\n",
    "    conv_1 = conv_layer(x, [3,3,3,32], [32], 2, 'VALID', 'conv_1')\n",
    "    conv_2 = conv_layer(conv_1, [3,3,32,32], [32], 1, 'VALID', 'conv_2')\n",
    "    conv_3 = conv_layer(conv_2, [3,3,32,64], [64], 1, 'SAME', 'conv_3')\n",
    "\n",
    "    pool_4 = max_pool(conv_3, [1,3,3,1], 2, 'VALID', 'pool_4') # 1\n",
    "    conv_4 = conv_layer(conv_3, [3,3,64,96], [96], 2, 'VALID', 'conv_4') # 2\n",
    "\n",
    "    concat_5 = tf.concat([pool_4, conv_4], axis=3, name='concat_5') # 73x73x160\n",
    "\n",
    "    conv_5_1 = conv_layer(concat_5, [1,1,160,64], [64], 1, 'SAME', 'conv_5_1') # 1\n",
    "    conv_6_1 = conv_layer(conv_5_1, [3,3,64,96], [96], 1, 'VALID', 'conv_6_1')\n",
    "\n",
    "    conv_5_2 = conv_layer(concat_5, [1,1,160,64], [64], 1, 'SAME', 'conv_5_2') # 2\n",
    "    conv_6_2 = conv_layer(conv_5_2, [7,1,64,64], [64], 1, 'SAME', 'conv_6_2')\n",
    "    conv_7_2 = conv_layer(conv_6_2, [1,7,64,64], [64], 1, 'SAME', 'conv_7_2')\n",
    "    conv_8_2 = conv_layer(conv_7_2, [3,3,64,96], [96], 1, 'VALID', 'conv_8_2')\n",
    "\n",
    "    concat_9 = tf.concat([conv_6_1, conv_8_2], axis=3, name='concat_9') # 71x71x192\n",
    "\n",
    "    conv_10 = conv_layer(concat_9, [3,3,192,192], [192], 2, 'VALID', 'conv_10') # 1\n",
    "    pool_10 = max_pool(concat_9, [1,2,2,1], 2, 'VALID', 'pool_10') # 1\n",
    "\n",
    "    concat_11 = tf.concat([conv_10, pool_10], axis=3, name='concat_11') # 71x71x192\n",
    "    # print(\"stem shape : \",concat_11.shape)\n",
    "\n",
    "    inception_a_12 = inception_A(concat_11, 384, 'inception_a_12')\n",
    "    inception_a_13 = inception_A(inception_a_12, 384, 'inception_a_13')\n",
    "    inception_a_14 = inception_A(inception_a_13, 384, 'inception_a_14')\n",
    "    inception_a_15 = inception_A(inception_a_14, 384, 'inception_a_15') # 4번 반복\n",
    "    # print(\"inception A shape : \",inception_a_15.shape)\n",
    "\n",
    "    reduction_a_16 = reduction_A(inception_a_15, 384, 'reduction_a_16')\n",
    "    # print(\"reduction A shape : \",reduction_a_16.shape)\n",
    "\n",
    "    inception_b_17 = inception_B(reduction_a_16, 1024, 'inception_b_17')\n",
    "    inception_b_18 = inception_B(inception_b_17, 1024, 'inception_b_18')\n",
    "    inception_b_19 = inception_B(inception_b_18, 1024, 'inception_b_19')\n",
    "    inception_b_20 = inception_B(inception_b_19, 1024, 'inception_b_20')\n",
    "    inception_b_21 = inception_B(inception_b_20, 1024, 'inception_b_21')\n",
    "    inception_b_22 = inception_B(inception_b_21, 1024, 'inception_b_22')\n",
    "    inception_b_23 = inception_B(inception_b_22, 1024, 'inception_b_23')\n",
    "    # print(\"inception B shape : \",inception_b_23.shape)\n",
    "\n",
    "    reduction_b_24 = reduction_B(inception_b_23, 1024, 'reduction_b_24')\n",
    "    # print(\"reduction B shape : \",reduction_b_24.shape)\n",
    "\n",
    "    inception_c_25 = inception_C(reduction_b_24, 1536, 'inception_c_25')\n",
    "    inception_c_26 = inception_C(inception_c_25, 1536, 'inception_c_26')\n",
    "    inception_c_27 = inception_C(inception_c_26, 1536, 'inception_c_27')\n",
    "    # print(\"inception C shape : \",inception_c_27.shape)\n",
    "\n",
    "    pool_28 = avg_pool(inception_c_27, [1,8,8,1], 1, 'VALID', 'avg_pool_28') # 1 - avg_pool\n",
    "\n",
    "    dropout = tf.nn.dropout(pool_28, keep_prob)\n",
    "    flatten = tf.reshape(dropout, [-1, 1*1*1536])\n",
    "\n",
    "    logits = fc_layer(flatten, 1536, img_result,'fc_layer')\n",
    "    print(\"logits : \",logits.shape)\n",
    "\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    #all을 하면 모든 이미지 다 가져온다. 뒤의 16은 한번 배치 할 때 가져올 크기\n",
    "    iterator, next_element = get_iterator_next_element('한과',8)\n",
    "\n",
    "    img_width = 299\n",
    "    img_height = 299\n",
    "    img_channel = 3\n",
    "    img_result = 150\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, img_width, img_height, img_channel])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, img_result])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    logits = dcnn(x, keep_prob, img_result)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.045)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(iterator.initializer)\n",
    "        for i in range(100):\n",
    "            image, label = sess.run(next_element)\n",
    "            if i%5 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={x: image, y: label, keep_prob:1.0})\n",
    "                train_loss = sess.run(loss, feed_dict={x: image, y: label, keep_prob:1.0})\n",
    "                print(\"\\nstep %d  : train accuracy : %.4f, train loss end : %.4f\"%(i,train_accuracy,train_loss))\n",
    "            sess.run(train_step, feed_dict={x: image, y: label, keep_prob: 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sfsfk\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py:332: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From C:\\Users\\sfsfk\\Desktop\\develope\\capstone\\imgFetch.py:68: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
      "batch is ready\n",
      "WARNING:tensorflow:From <ipython-input-1-151516459eaa>:82: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "logits :  (?, 150)\n",
      "WARNING:tensorflow:From <ipython-input-2-e48ce5af36bd>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\sfsfk\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "step 0  : train accuracy : 0.0000, train loss end : 5.0106\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
