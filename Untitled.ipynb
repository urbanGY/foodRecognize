{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sfsfk\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12943216662743688962\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1422723891\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9996497221954287014\n",
      "physical_device_desc: \"device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "tensorflow version :  2.0.0\n",
      "is gpu available??? pease.. :  True\n"
     ]
    }
   ],
   "source": [
    "import imgFetch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from layers.conv_layer import conv_layer\n",
    "from layers.max_pool import max_pool\n",
    "from layers.fc_layer import fc_layer\n",
    "\n",
    "from layers.inception import inception_A, inception_B, inception_C\n",
    "from layers.reduction import reduction_A, reduction_B\n",
    "from layers.avg_pool import avg_pool\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print('tensorflow version : ', tf.__version__)\n",
    "print('is gpu available??? pease.. : ',tf.test.is_gpu_available())\n",
    "\n",
    "def showList(img_list, label_list):\n",
    "    print(\"\\n**************************\")\n",
    "    print(\"image path list length : \",len(img_list))\n",
    "    print(\"label list length : \",len(label_list))\n",
    "    print(\"test case 0\")\n",
    "    print(img_list[0])\n",
    "    print(label_list[0])\n",
    "    print(\" \")\n",
    "\n",
    "def _read_py_function(path, label):\n",
    "    img = np.array(Image.open(path))\n",
    "    label = np.array(label, dtype=np.float32)\n",
    "    return img.astype(np.float32), label\n",
    "\n",
    "def get_iterator_next_element():\n",
    "    img_list, label_list = imgFetch.get_path_label(\"한과\")\n",
    "\n",
    "    batch_size = 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_list, label_list))\n",
    "    dataset = dataset.map(lambda img_list, label_list: tuple(tf.py_func(_read_py_function,[img_list, label_list], [tf.float32, tf.float32])))\n",
    "\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=(int(len(img_list) *0.4) + 3 * batch_size))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    print(\"batch is ready\")\n",
    "    return iterator, next_element\n",
    "\n",
    "# reuse를 사용하면 재사용이 된다. 그런데 여기에 그게 들어가는게 맞는지 모르겠음;\n",
    "# graph로 variable보는법 알아서 나중에 어떻게 돌아가나 판단\n",
    "\n",
    "# 여긴 나중에 다시 체크하는게 좋을 듯\n",
    "def dcnn(x):\n",
    "    # stem\n",
    "    conv_1 = conv_layer(x, [3,3,3,32], [32], 2, 'VALID', 'conv_1')\n",
    "    conv_2 = conv_layer(conv_1, [3,3,32,32], [32], 1, 'VALID', 'conv_2')\n",
    "    conv_3 = conv_layer(conv_2, [3,3,32,64], [64], 1, 'SAME', 'conv_3')\n",
    "\n",
    "    pool_4 = max_pool(conv_3, [1,3,3,1], 2, 'VALID', 'pool_4') # 1\n",
    "    conv_4 = conv_layer(conv_3, [3,3,64,96], [96], 2, 'VALID', 'conv_4') # 2\n",
    "\n",
    "    concat_5 = tf.concat([pool_4, conv_4], axis=3, name='concat_5') # 73x73x160\n",
    "\n",
    "    conv_5_1 = conv_layer(concat_5, [1,1,160,64], [64], 1, 'SAME', 'conv_5_1') # 1\n",
    "    conv_6_1 = conv_layer(conv_5_1, [3,3,64,96], [96], 1, 'VALID', 'conv_6_1')\n",
    "\n",
    "    conv_5_2 = conv_layer(concat_5, [1,1,160,64], [64], 1, 'SAME', 'conv_5_2') # 2\n",
    "    conv_6_2 = conv_layer(conv_5_2, [7,1,64,64], [64], 1, 'SAME', 'conv_6_2')\n",
    "    conv_7_2 = conv_layer(conv_6_2, [1,7,64,64], [64], 1, 'SAME', 'conv_7_2')\n",
    "    conv_8_2 = conv_layer(conv_7_2, [3,3,64,96], [96], 1, 'VALID', 'conv_8_2')\n",
    "\n",
    "    concat_9 = tf.concat([conv_6_1, conv_8_2], axis=3, name='concat_9') # 71x71x192\n",
    "\n",
    "    conv_10 = conv_layer(concat_9, [3,3,192,192], [192], 2, 'VALID', 'conv_10') # 1\n",
    "    pool_10 = max_pool(concat_9, [1,2,2,1], 2, 'VALID', 'pool_10') # 1\n",
    "\n",
    "    concat_11 = tf.concat([conv_10, pool_10], axis=3, name='concat_11') # 71x71x192\n",
    "    print(\"concat 11 shape : \",concat_11.shape)\n",
    "\n",
    "    inception_a_12 = inception_A(concat_11, 384, 'inception_a_12') # 4번 반복\n",
    "    print(\"inception_a_12 shape : \",inception_a_12.shape)\n",
    "\n",
    "    reduction_a_13 = reduction_A(inception_a_12, 384, 'reduction_a_13')\n",
    "    print(\"reduction_a_13 shape : \",reduction_a_13.shape)\n",
    "\n",
    "    inception_b_13 = inception_B(reduction_a_13, 1024, 'inception_b_13')\n",
    "    print(\"inception_b_13 shape : \",inception_b_13.shape)\n",
    "\n",
    "    reduction_b_14 = reduction_B(inception_b_13, 1024, 'reduction_b_14')\n",
    "    print(\"reduction_b_14 shape : \",reduction_b_14.shape)\n",
    "\n",
    "    inception_c_14 = inception_C(reduction_b_14, 1536, 'inception_c_14')\n",
    "    print(\"inception_c_14 shape : \",inception_c_14.shape)\n",
    "    return concat_11\n",
    "\n",
    "def test():\n",
    "    iterator, next_element = get_iterator_next_element()\n",
    "\n",
    "    img_width = 299\n",
    "    img_height = 299\n",
    "    img_channel = 3\n",
    "    img_result = 150\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, img_width, img_height, img_channel])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, img_result])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    test = dcnn(x)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(iterator.initializer)\n",
    "        for i in range(3):\n",
    "            image, label = sess.run(next_element)\n",
    "            _t = sess.run(test, feed_dict={x:image})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sfsfk\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py:332: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From <ipython-input-1-feb53e474ec2>:47: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
      "batch is ready\n",
      "concat 11 shape :  (?, 35, 35, 384)\n",
      "inception_a_12 shape :  (?, 35, 35, 384)\n",
      "reduction_a_13 shape :  (?, 17, 17, 1024)\n",
      "inception_b_13 shape :  (?, 17, 17, 1024)\n",
      "reduction_b_14 shape :  (?, 8, 8, 1536)\n",
      "inception_c_14 shape :  (?, 8, 8, 1536)\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
